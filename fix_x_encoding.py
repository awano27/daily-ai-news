#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
XæŠ•ç¨¿ã®æ–‡å­—åŒ–ã‘ä¿®æ­£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
CSVã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æ™‚ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œã‚’è§£æ±º
"""

import requests
import csv
from io import StringIO
import html
import re

def fix_csv_encoding():
    """CSVãƒ‡ãƒ¼ã‚¿ã®æ–‡å­—åŒ–ã‘ä¿®æ­£ã¨ãƒ†ã‚¹ãƒˆ"""
    csv_url = "https://docs.google.com/spreadsheets/d/1uuLKCLIJw--a1vCcO6UGxSpBiLTtN8uGl2cdMb6wcfg/export?format=csv&gid=0"
    
    print("ğŸ“‹ CSVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ãƒ†ã‚¹ãƒˆ...")
    
    try:
        # CSVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        response = requests.get(csv_url, timeout=30)
        response.raise_for_status()
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ˜ç¤ºçš„ã«è¨­å®š
        response.encoding = 'utf-8'
        csv_content = response.text
        
        print(f"âœ… CSVãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ ({len(csv_content)} æ–‡å­—)")
        
        # CSVè§£æ
        csv_file = StringIO(csv_content)
        reader = csv.DictReader(csv_file)
        
        posts = []
        for i, row in enumerate(reader):
            if i >= 5:  # æœ€åˆã®5æŠ•ç¨¿ã®ã¿ãƒ†ã‚¹ãƒˆ
                break
                
            content = row.get('Tweet', row.get('Content', ''))
            url = row.get('URL', row.get('Link', ''))
            
            if content and url:
                # HTML ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
                content_cleaned = html.unescape(content)
                
                # æ”¹è¡Œæ–‡å­—ã‚’æ­£è¦åŒ–
                content_cleaned = re.sub(r'[\r\n]+', ' ', content_cleaned)
                
                # ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
                content_cleaned = re.sub(r'\s+', ' ', content_cleaned).strip()
                
                posts.append({
                    'content': content_cleaned,
                    'url': url,
                    'raw_content': content[:100] + '...' if len(content) > 100 else content
                })
                
                print(f"\nğŸ”¤ æŠ•ç¨¿ {i+1}:")
                print(f"  å…ƒãƒ‡ãƒ¼ã‚¿: {content[:50]}...")
                print(f"  ä¿®æ­£å¾Œ: {content_cleaned[:50]}...")
                print(f"  URL: {url}")
        
        print(f"\nğŸ“Š å‡¦ç†çµæœ: {len(posts)} æŠ•ç¨¿ã‚’æ­£å¸¸ã«å‡¦ç†")
        
        # ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’HTMLã«åæ˜ 
        update_html_with_fixed_posts(posts)
        
        return posts
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return []

def update_html_with_fixed_posts(posts):
    """ä¿®æ­£ã•ã‚ŒãŸXæŠ•ç¨¿ã§HTMLã‚’æ›´æ–°"""
    try:
        with open('index.html', 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        # XæŠ•ç¨¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
        posts_start = html_content.find('data-category="Posts"')
        if posts_start == -1:
            print("âš ï¸ Posts ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
            
        # æ–‡å­—åŒ–ã‘ã—ãŸæ—¥æœ¬èªæŠ•ç¨¿ã‚’ä¿®æ­£ã•ã‚ŒãŸã‚‚ã®ã«ç½®æ›
        for i, post in enumerate(posts):
            if 'æ—¥æœ¬èª' in post['content'] or 'AI' in post['content']:
                # ç‰¹å®šã®æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¿®æ­£
                original_garbled = post['raw_content']
                fixed_content = post['content']
                
                # HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„å†…ã®å¯¾å¿œã™ã‚‹æŠ•ç¨¿ã‚’æ›´æ–°
                html_content = html_content.replace(original_garbled[:30], fixed_content[:100])
        
        # ä¿®æ­£ã•ã‚ŒãŸHTMLã‚’ä¿å­˜
        with open('index.html', 'w', encoding='utf-8') as f:
            f.write(html_content)
            
        print("âœ… index.html ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä¿®æ­£å®Œäº†")
        
    except Exception as e:
        print(f"âŒ HTMLæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    print("ğŸ”§ XæŠ•ç¨¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä¿®æ­£é–‹å§‹...")
    posts = fix_csv_encoding()
    if posts:
        print("âœ… ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä¿®æ­£å®Œäº†ï¼")
    else:
        print("âŒ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä¿®æ­£å¤±æ•—")